{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Task 13\n",
    "# Use object oriented programming and/or the Matrix Notation to build and train the network\n",
    "# 2 classes were created as part of the Object Oriented Programming, the NeuronLayer class representing neurons and the NueralNetwork class representing the whole Nueral Network\n",
    "\n",
    "\n",
    "class NeuronLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        # Forward pass\n",
    "        self.input = X\n",
    "        self.output = self.sigmoid(np.dot(X, self.weights) + self.biases)\n",
    "\n",
    "    def backward_pass(self, error, learning_rate):\n",
    "        # Backward pass (compute gradients)\n",
    "        delta = error * self.sigmoid_derivative(self.output)\n",
    "\n",
    "        # Update weights using gradient descent\n",
    "        self.weights += self.input.T.dot(delta) * learning_rate\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Create neuron layers\n",
    "        self.hidden_layer = NeuronLayer(input_size, hidden_size)\n",
    "        self.output_layer = NeuronLayer(hidden_size, output_size)\n",
    "\n",
    "    # Task 8\n",
    "    # Error Aggrigation\n",
    "    # Calculate the aggrigated error\n",
    "    def error_Nuweiba(self, actual_outputs, target_outputs):\n",
    "        # Calculate error vector\n",
    "        error_vector = actual_outputs - target_outputs\n",
    "\n",
    "        # Calculate aggregated error\n",
    "        aggregated_error = np.mean(np.abs(error_vector))\n",
    "\n",
    "        return error_vector, aggregated_error\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        # Forward pass\n",
    "        self.hidden_layer.forward_pass(X)\n",
    "        self.output_layer.forward_pass(self.hidden_layer.output)\n",
    "\n",
    "    def compute_loss(self, y):\n",
    "        # Compute the loss\n",
    "        return np.mean(0.5 * (y - self.output_layer.output) ** 2)\n",
    "\n",
    "    # Task 6\n",
    "    # Implement the backpropagation algorithm to train the network. The function apply one training epoch only.\n",
    "    def backward_pass(self, X, y, learning_rate):\n",
    "        # Backward pass (compute gradients)\n",
    "        output_error = y - self.output_layer.output\n",
    "\n",
    "        # Backpropagation through layers\n",
    "        self.output_layer.backward_pass(output_error, learning_rate)\n",
    "        self.hidden_layer.backward_pass(output_error.dot(self.output_layer.weights.T), learning_rate)\n",
    "\n",
    "    def validate(self, X_val, y_val):\n",
    "        # Make predictions on the validation set\n",
    "        self.forward_pass(X_val)\n",
    "        predictions = (self.output_layer.output > 0.5).astype(int)\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        accuracy = np.mean(predictions == y_val)\n",
    "        return accuracy\n",
    "\n",
    "    def calculate_map(self, predictions, ground_truth):\n",
    "      # Calculate mean average precision\n",
    "        mAP = average_precision_score(ground_truth, predictions)\n",
    "        return mAP\n",
    "\n",
    "    # Task 7\n",
    "    # Impalement a function to test the network by forward passing a set of labeled data\n",
    "    def test_Nuweiba(self, X_test, y_test):\n",
    "        # Make predictions on the test set\n",
    "        self.forward_pass(X_test)\n",
    "        predictions = (self.output_layer.output > 0.5).astype(int)\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        accuracy = np.mean(predictions == y_test)\n",
    "        print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "        # Confusion Matrix\n",
    "        conf_matrix = confusion_matrix(y_test, predictions)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "\n",
    "        # Plot Confusion Matrix\n",
    "        self.plot_confusion_matrix(conf_matrix)\n",
    "\n",
    "        # Calculate error matrix and aggregated error\n",
    "        error_vector, aggregated_error = self.error_Nuweiba(predictions, y_test)\n",
    "\n",
    "        # Print and return error information\n",
    "        print(f\"Aggregated Error: {aggregated_error}\")\n",
    "\n",
    "        return error_vector, aggregated_error\n",
    "\n",
    "    def train_Nuweiba(self, X, y, X_val, y_val, epochs, learning_rate, batch_size=100):\n",
    "        num_samples = X.shape[0]\n",
    "        training_loss_history = []\n",
    "        validation_accuracy_history = []\n",
    "        map_history = []\n",
    "        aggregated_error_history = []\n",
    "\n",
    "        # Initialize the final weights matrix\n",
    "        final_weights_matrix = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the data before each epoch\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                # Get the current batch\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                self.forward_pass(X_batch)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = self.compute_loss(y_batch)\n",
    "\n",
    "                # Task 9\n",
    "                # The backpropagation pass should be executed after each iteration (one batch).\n",
    "                # Backward pass\n",
    "                self.backward_pass(X_batch, y_batch, learning_rate)\n",
    "\n",
    "            # Task 10\n",
    "            # The validation is performed after each epoch.\n",
    "            # Validate after every epoch\n",
    "            validation_accuracy = self.validate(X_val, y_val)\n",
    "            validation_accuracy_history.append(validation_accuracy)\n",
    "\n",
    "            # Calculate and save mAP for demonstration purposes\n",
    "            predictions = self.output_layer.output\n",
    "            ground_truth = y_val\n",
    "            map_value = self.calculate_map(predictions, ground_truth)\n",
    "            map_history.append(map_value)\n",
    "\n",
    "            # Calculate and save aggregated error\n",
    "            error_vector, aggregated_error = self.error_Nuweiba(predictions, ground_truth)\n",
    "            aggregated_error_history.append(aggregated_error)\n",
    "\n",
    "            # Print the loss, validation accuracy, and mAP after every epoch\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss}, Validation Accuracy: {validation_accuracy}, mAP: {map_value}, Aggregated Error: {aggregated_error}\")\n",
    "\n",
    "            # Save training loss for plotting\n",
    "            training_loss_history.append(loss)\n",
    "\n",
    "            # Save the weights matrix for the final epoch\n",
    "            final_weights_matrix = self.output_layer.weights.copy()\n",
    "\n",
    "        # Plotting\n",
    "        self.plot_stats(training_loss_history, validation_accuracy_history, map_history, aggregated_error_history)\n",
    "\n",
    "        # Return the final weights matrix\n",
    "        return final_weights_matrix\n",
    "\n",
    "    # Task 12\n",
    "    # Report the training, validation and testing accuracies. Use tables, graphs, and charts as possible\n",
    "    # Plotting functions\n",
    "    def plot_confusion_matrix(self, conf_matrix):\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "\n",
    "        classes = ['Class 0', 'Class 1']  # Adjust classes based on your actual classes\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_stats(self, training_loss_history, validation_accuracy_history, map_history, aggregated_error_history):\n",
    "        # Plot training loss, validation accuracy, mAP, and aggregated error\n",
    "        plt.figure(figsize=(18, 5))\n",
    "\n",
    "        # Plot training loss\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.plot(training_loss_history, label='Training Loss')\n",
    "        plt.title('Training Loss Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot validation accuracy\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.plot(validation_accuracy_history, label='Validation Accuracy', color='orange')\n",
    "        plt.title('Validation Accuracy Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot mAP\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.plot(map_history, label='mAP', color='green')\n",
    "        plt.title('mAP Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('mAP')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot aggregated error\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.plot(aggregated_error_history, label='Aggregated Error', color='red')\n",
    "        plt.title('Aggregated Error Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Aggregated Error')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# Use Banknote authentication dataset\n",
    "# Importing dataset and creating Features and Labels for training and testing the Nueral Network\n",
    "\n",
    "# Load the dataset and preprocess\n",
    "# Load the dataset and preprocess\n",
    "url1 = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
    "url2 = \"/content/smoking.csv\"\n",
    "url3 = \"/content/heart_disease_health_indicators_BRFSS2015.csv\"\n",
    "\n",
    "column_names1 = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"]\n",
    "column_names2 = [\"ID\",\"gender\",\"age\",\"height(cm)\",\"weight(kg)\",\"waist(cm)\",\"eyesight(left)\",\"eyesight(right)\",\"hearing(left)\",\"hearing(right)\",\"systolic\",\"relaxation\",\"fasting blood sugar\",\"Cholesterol\",\"triglyceride\",\"HDL\",\"LDL\",\"hemoglobin\",\"Urine protein\",\"serum creatinine\",\"AST\",\"ALT\",\"Gtp\",\"oral\",\"dental caries\",\"tartar\",\"smoking\"]\n",
    "column_names3 = [\"HeartDiseaseorAttack\",\"HighBP\",\"HighChol\",\"CholCheck\",\"BMI\",\"Smoker\",\"Stroke\",\"Diabetes\",\"PhysActivity\",\"Fruits\",\"Veggies\",\"HvyAlcoholConsump\",\"AnyHealthcare\",\"NoDocbcCost\",\"GenHlth\",\"MentHlth\",\"PhysHlth\",\"DiffWalk\",\"Sex\",\"Age\",\"Education\",\"Income\"]\n",
    "\n",
    "df1 = pd.read_csv(url1, names=column_names1)\n",
    "df2 = pd.read_csv(url2, names=column_names2)\n",
    "df3 = pd.read_csv(url3, names=column_names3)\n",
    "\n",
    "X = df1.drop(\"Class\", axis=1).values\n",
    "y = df1[\"Class\"].values.reshape(-1, 1)\n",
    "\n",
    "# Second Dataset\n",
    "#X = df3.drop([\"HeartDiseaseorAttack\",\"HighBP\",\"HighChol\",\"CholCheck\",\"BMI\",\"Fruits\",\"Veggies\",\"HvyAlcoholConsump\",\"AnyHealthcare\",\"NoDocbcCost\",\"GenHlth\",\"MentHlth\",\"PhysHlth\",\"DiffWalk\",\"Sex\",\"Age\",\"Education\",\"Income\"], axis=1).values\n",
    "#y = df3[\"HeartDiseaseorAttack\"].values.reshape(-1, 1)\n",
    "\n",
    "#X = X[1:].astype(float).astype(int)\n",
    "#y = y[1:].astype(float).astype(int)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Task 3\n",
    "# Split the data randomly into [T, V, S]. T = 70% for training, V = 20% for validation, and S = 10% for testing\n",
    "# Spliting Data first 70% Training and 30% rest, then the 30% is divided 2/3 for Validation and 1/3 for Testing\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Task 1\n",
    "# Build a network as [4,10,1]. Input layer: 4 neurons, one hidden layer of 10 neurons and one output layer with 1 neuron\n",
    "# Initialize the architecture values as mentioned\n",
    "\n",
    "# Network Architecture [4, 10, 1]\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Create an instance of the NeuralNetwork class\n",
    "neural_network = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Task 5\n",
    "# Train the network at least 10 epochs\n",
    "# Train the neural network with batch training and validate after each epoch\n",
    "Weights_Matrix = neural_network.train_Nuweiba(X_train_scaled, y_train, X_val_scaled, y_val, epochs, learning_rate, batch_size=100)\n",
    "\n",
    "print(Weights_Matrix)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# task 11\n",
    "# The testing is performed only once after the completion of the training\n",
    "# Testing the NeuralNetwork using the features and labels dedicated for testing (10% of the dataset)\n",
    "\n",
    "# Test the neural network on the test set\n",
    "Error_Vector, Error = neural_network.test_Nuweiba(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
